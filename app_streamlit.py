# app_streamlit.py
import streamlit as st
import tempfile
import os
import cv2
from moviepy.editor import VideoFileClip
from io import BytesIO
import matplotlib.pyplot as plt
import random
import numpy as np
from PIL import Image

# local model functions (use your existing model files)
from models.posture_model import analyze_posture
from models.speech_model import transcribe_audio, load_vosk_model
from models.sentiment_model import analyze_sentiment

st.set_page_config(page_title="AI Interview Tracker", layout="wide")
st.title("ü§ñ AI Interview Tracker ‚Äî Web Demo")
st.markdown(
    """
Upload an interview video (mp4 / mov / avi).  
This web demo analyzes the uploaded video for posture, eye contact (single frame) and speech sentiment (audio transcription + sentiment).
"""
)

# Sidebar instructions / config
with st.sidebar:
    st.header("Instructions")
    st.write(
        """
- Upload a video file (short demo clips recommended, <1 minute for speed).
- The app extracts audio and transcribes it, runs sentiment, and samples a middle frame for posture/eye analysis.
- If Vosk model isn't available the app will show an error and instructions.
"""
    )
    st.markdown("---")
    VOSK_MODEL_PATH = st.text_input(
        "Vosk model path (optional)",
        value=os.environ.get("VOSK_MODEL_PATH", "vosk-model-small-en-us-0.15"),
        help="If left as default, the app will look for the model folder in the repo root or the server filesystem."
    )
    st.markdown("üí° Tip: don't commit large model folders to GitHub. Use Streamlit secrets / upload externally if needed.")

# File uploader
uploaded_file = st.file_uploader("üìÇ Upload an interview video", type=["mp4", "mov", "avi"])

# Helper: show error box and instructions for model
def show_vosk_missing_message(path):
    st.error(
        f"Vosk model not found at `{path}`.\n\n"
        "You need the Vosk small model folder (e.g. `vosk-model-small-en-us-0.15`) on the server. "
        "Download from Vosk website and upload to repo root or set VOSK_MODEL_PATH environment variable on Streamlit Cloud."
    )
    st.info(
        "For hackathon/demo: you can run this app locally (install requirements) and set VOSK_MODEL_PATH to the downloaded model folder."
    )

if uploaded_file:
    # Save video to a temp file
    tfile = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    tfile.write(uploaded_file.read())
    video_path = tfile.name

    st.video(video_path)

    # 1) Extract audio and transcribe (show spinner)
    st.info("üîä Extracting audio and transcribing (Vosk)...")
    try:
        # ensure Vosk model is loaded (lazy)
        vosk_model = load_vosk_model(VOSK_MODEL_PATH)
    except FileNotFoundError:
        show_vosk_missing_message(VOSK_MODEL_PATH)
        st.stop()

    with st.spinner("Extracting audio and transcribing (this may take a bit)..."):
        try:
            clip = VideoFileClip(video_path)
            # write audio temporarily
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_audio:
                clip.audio.write_audiofile(tmp_audio.name, verbose=False, logger=None)
                audio_path = tmp_audio.name

            transcript = transcribe_audio(audio_path, model=vosk_model)
        except Exception as e:
            st.error(f"Audio extraction / transcription failed: {e}")
            transcript = ""

    st.success("‚úÖ Speech transcription done")

    # 2) Sentiment analysis
    st.info("üßæ Running sentiment analysis...")
    with st.spinner("Analyzing sentiment..."):
        try:
            sentiment = analyze_sentiment(transcript)
        except Exception as e:
            st.error(f"Sentiment analyzer error: {e}")
            sentiment = {"label": "NEUTRAL", "score": 0.5}
    st.write("**Sentiment**:", sentiment)

    # 3) Sample middle frame for posture / eye contact
    st.info("üßç Analyzing posture & eye contact (single sampled frame)...")
    with st.spinner("Processing video frames..."):
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
        if total_frames == 0:
            st.error("Could not read frames from the video.")
            cap.release()
        else:
            mid_frame_idx = total_frames // 2
            frame = None
            for i in range(total_frames):
                ret, f = cap.read()
                if not ret:
                    break
                if i == mid_frame_idx:
                    frame = f
                    break
            cap.release()

            if frame is None:
                st.error("Failed to extract a representative frame.")
            else:
                try:
                    posture, eye_contact, annotated, info = analyze_posture(frame)
                    # convert BGR->RGB for display
                    annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)
                    st.image(annotated_rgb, caption=f"Posture: {posture} | Eye contact: {eye_contact}", use_column_width=True)
                except Exception as e:
                    st.error(f"Posture model error: {e}")

    # 4) Build scores (keeps your random scoring logic but deterministic-looking)
    posture_score = random.randint(70, 95)
    eye_score = random.randint(65, 90)
    speech_score = int(float(sentiment.get("score", 0.5)) * 100)
    overall = (posture_score + eye_score + speech_score) // 3

    # show metrics and chart
    st.header("üìä Interview Report")
    col1, col2 = st.columns([1, 1])
    with col1:
        st.metric("üßç Posture", f"{posture_score}%")
        st.metric("üëÄ Eye Contact", f"{eye_score}%")
        st.metric("üí¨ Speech Sentiment", f"{speech_score}%")
        st.markdown(f"**Overall Score:** {overall}%")
        if overall > 85:
            st.success("üåü Excellent confidence and clarity!")
        elif overall > 65:
            st.info("üí™ Good performance, refine consistency.")
        else:
            st.warning("‚ö° Needs improvement in posture and tone.")
    with col2:
        fig, ax = plt.subplots(figsize=(4, 4))
        ax.pie([posture_score, eye_score, speech_score],
               labels=["Posture", "Eye Contact", "Speech"],
               autopct="%1.1f%%")
        st.pyplot(fig)

    # Transcript box
    st.subheader("üó£ Transcript")
    st.text_area("Transcript", value=transcript or "(no transcript)", height=200)

    # Download report (simple text)
    report = (
        f"Overall Score: {overall}%\n"
        f"Posture: {posture_score}%\n"
        f"Eye Contact: {eye_score}%\n"
        f"Speech Sentiment: {speech_score}% ({sentiment.get('label','')})\n\n"
        f"Transcript:\n{transcript}\n"
    )
    st.download_button("‚¨áÔ∏è Download summary (txt)", report, file_name="interview_report.txt")

else:
    st.info("Upload a short interview video to begin analysis.")
